{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnil-mergal/HEALTH-INSURANCE-CROSS-SELL-PREDICTION-PROJECT/blob/main/Individual_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HEALTH INSURANCE CROSS SELL PREDICTION"
      ],
      "metadata": {
        "id": "v2IDVfBxDViw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcl5Bv9ed6u"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJAmGx75jJk1"
      },
      "source": [
        "Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n",
        "\n",
        "An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n",
        "\n",
        "For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n",
        "\n",
        "Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.\n",
        "\n",
        "Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
        "\n",
        "Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzGDqdC4fZ-b"
      },
      "source": [
        "# **Attribute Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJxl68MjYbs"
      },
      "source": [
        "1. id :\tUnique ID for the customer\n",
        "\n",
        "2. Gender\t: Gender of the customer\n",
        "\n",
        "3. Age :\tAge of the customer\n",
        "\n",
        "4. Driving_License\t0 : Customer does not have DL, 1 : Customer already has DL\n",
        "\n",
        "5. Region_Code :\tUnique code for the region of the customer\n",
        "\n",
        "6. Previously_Insured\t: 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
        "\n",
        "7. Vehicle_Age :\tAge of the Vehicle\n",
        "\n",
        "8. Vehicle_Damage\t :1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
        "\n",
        "9. Annual_Premium\t: The amount customer needs to pay as premium in the year\n",
        "\n",
        "10. PolicySalesChannel :\tAnonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
        "\n",
        "11. Vintage :\tNumber of Days, Customer has been associated with the company\n",
        "\n",
        "12. Response :\t1 : Customer is interested, 0 : Customer is not interested"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "ps0GjhVBdR_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W7Rl8yEDDtY"
      },
      "outputs": [],
      "source": [
        "# Essential Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "XN4MGac532Oy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGDq6hP7DDvY"
      },
      "outputs": [],
      "source": [
        "#reading dataset\n",
        "df1=pd.read_csv('/content/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.CSV')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "pe_1Oaq7EdYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72KhJ7d-DDyn"
      },
      "outputs": [],
      "source": [
        "#Descriptive statistics\n",
        "df1.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZHpMad6DD2W"
      },
      "outputs": [],
      "source": [
        "#Shape of data\n",
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_VMYhiCDD5T"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TjVmkV_I-wC"
      },
      "source": [
        "* Our dataset have\n",
        " no null value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kqrd008JTnA"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "nV_R0TaS4LmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# missing values"
      ],
      "metadata": {
        "id": "MXZn3p3x4cac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values:\\n\", df1.isnull().sum())\n"
      ],
      "metadata": {
        "id": "oVAYj7hb4P41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "rvdgr5kr5DSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correlation heatmap"
      ],
      "metadata": {
        "id": "maMT477X4iGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical columns to numerical using Label Encoding or One-Hot Encoding\n",
        "\n",
        "# Select categorical columns with string values\n",
        "categorical_cols = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\n",
        "\n",
        "# Import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in categorical_cols:\n",
        "    df1[col] = label_encoder.fit_transform(df1[col])\n",
        "\n",
        "# Now you can create the correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df1.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6muHqrZy4Yqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Target Variable"
      ],
      "metadata": {
        "id": "x3aTT-xV5Vzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df1.sample(n=10000, random_state=42)  # 10,000 random rows\n",
        "sns.countplot(x=df_sample['Response'])\n",
        "plt.title(\"Distribution of Target Variables (Sampled)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ye5Zyh015Nob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vehicle Age Distribution"
      ],
      "metadata": {
        "id": "SsxCyojg5mFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(df1['Vehicle_Age'])\n",
        "plt.title(\"Vehicle Age Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vAYUxM5y5rJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Preprocess Data\n",
        "# Encoding Categorical Variables"
      ],
      "metadata": {
        "id": "jIuHUTxi5yc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode 'Gender', 'Vehicle_Age', 'Vehicle_Damage'\n",
        "le = LabelEncoder()\n",
        "df1['Gender'] = le.fit_transform(df1['Gender'])\n",
        "df1['Vehicle_Age'] = df1['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})\n",
        "df1['Vehicle_Damage'] = le.fit_transform(df1['Vehicle_Damage'])\n"
      ],
      "metadata": {
        "id": "w_1n6Q0T53oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "cnfC9kVx5_1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "numerical_features = ['Age', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
        "df1[numerical_features] = scaler.fit_transform(df1[numerical_features])\n"
      ],
      "metadata": {
        "id": "emejKJJQ6DhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "NkwDuU246Huo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and Target\n",
        "X = df1.drop(columns=['Response'])\n",
        "y = df1['Response']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "E73zP8bb6QeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Implement Algorithms\n",
        "1. Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "V7RvDqtE6l6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "id": "DrGi9X8Y6qzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Random Forest"
      ],
      "metadata": {
        "id": "6WWEBUxO6waD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "H8uD0QvQ608v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clean Column Names\n",
        "df1.columns = df1.columns.str.strip()\n",
        "\n",
        "# Use a Subset of the Data for Faster Execution (Optional)\n",
        "df1_sample = df1.sample(n=10000, random_state=42)  # Sample 10,000 rows for testing\n",
        "\n",
        "# Define Features and Target Variable\n",
        "X = df1_sample.drop(['Response', 'id'], axis=1, errors='ignore')  # Drop target & ID columns\n",
        "y = df1_sample['Response']\n",
        "\n",
        "# Split Data into Training and Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the Data for Faster SVM Convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Linear SVM (Faster Alternative to SVC)\n",
        "svm_model = LinearSVC(max_iter=1000, random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the Model\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ZyQRZye7JQwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Support Vector Machine"
      ],
      "metadata": {
        "id": "m1OnjMjD665C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "hBFsDIPm6_xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Decision Tree"
      ],
      "metadata": {
        "id": "nL4Sz2hX7B65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "4cj73djX7Hhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.XG Boost"
      ],
      "metadata": {
        "id": "Jyy6yyCy7Jsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "HcrIHfGO7MG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate Models\n",
        "# Confusion Matrix and Classification Report\n",
        "\n"
      ],
      "metadata": {
        "id": "qcgsU8vu7Tqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "models = {'Logistic Regression': lr, 'Random Forest': rf, 'SVM': svm, 'Decision Tree': dt, 'XGBoost': xgb}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model.predict(X_test)))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "9eFKqeHv7ZvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC CURVE"
      ],
      "metadata": {
        "id": "YI0iyhTu7i2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, model in models.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    plt.plot(fpr, tpr, label=name)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3I5vWLug7nrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Save and Export Results"
      ],
      "metadata": {
        "id": "uW0Ux8W97w0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model predictions\n",
        "output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_xgb})\n",
        "output.to_csv(\"predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "HucP4YyP706X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6RHZvkGJTph"
      },
      "outputs": [],
      "source": [
        "#Dependent variable 'Response'\n",
        "plt.figure(figsize=(8,7))\n",
        "sns.set_theme(style='whitegrid')\n",
        "sns.countplot(x=df1['Response'],data=df1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aisfwXRM5o5C"
      },
      "source": [
        "* From above fig we can see that the data is highly imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokK-zFLJTso"
      },
      "outputs": [],
      "source": [
        "#Distribution of Age\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.countplot(x=df1['Age'],data=df1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlDtkxcD5pl7"
      },
      "source": [
        "* From the above distribution of age we can see that most of the customers age is between 21 to 25 years.There are few Customers above the age of 60 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPcPEuuWJTwE"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(7,9))\n",
        "plt.pie(df1['Previously_Insured'].value_counts(), autopct='%.0f%%', shadow=True, startangle=200, explode=[0.01,0])\n",
        "plt.legend(labels=['Insured','Not insured'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4kcB2rRGZs"
      },
      "source": [
        "* 54% customer are previously insured ahe 46% customer are are not insured yet.\n",
        "* Customer who are not perviosly insured are likely to be inetrested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ7EiUDDMmC8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,9))\n",
        "a=df1['Annual_Premium']\n",
        "sns.distplot(a, color='purple')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the distribution plot we can infer that the annual premimum variable is right skewed"
      ],
      "metadata": {
        "id": "zhTqJ-Ir1Pdl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Lw2hKDMmJC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(df1['Annual_Premium'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYecW7Zc39fC"
      },
      "source": [
        "*  For the boxplot above we can see that there's a lot of outliers in the annual premium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmfWVhcFwbUg"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5,7))\n",
        "sns.countplot(x=df1['Vehicle_Damage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Customers with Vehicle_Damage are likely to buy insurance"
      ],
      "metadata": {
        "id": "Iyz45Zch1yBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTZCMa_xwbX5"
      },
      "outputs": [],
      "source": [
        "df1['Vehicle_Age'].hist();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the above plot we can see that most of the people are having vehicle age between 1 or 2 years and very few people are having vehicle age more than 2 years."
      ],
      "metadata": {
        "id": "k2PX1SwepRBr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vEshKR8GClh"
      },
      "source": [
        "#Bivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qfdFp51F9w9"
      },
      "outputs": [],
      "source": [
        "#Age VS Response\n",
        "plt.figure(figsize=(16,8))\n",
        "sns.countplot(data=df1, x='Age',hue='Response', palette='CMRmap_r')\n",
        "plt.xlabel('Age response')\n",
        "plt.ylabel('count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlCRh6HFquJL"
      },
      "source": [
        "* People ages between from 31 to 50 are more likely to respond.\n",
        "*  while Young people below 30 are not interested in vehicle insurance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vC4W-IMF93F"
      },
      "outputs": [],
      "source": [
        "#Gender vs Response\n",
        "df1.groupby(['Gender', 'Response']).size().unstack().plot(kind = 'bar', stacked = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Male category is slightly greater than that of female and chances of buying the insurance is also little high"
      ],
      "metadata": {
        "id": "rZKyC-rJf_ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6) )\n",
        "sns.countplot(data = df1, x = 'Vehicle_Age', hue = 'Response', palette='Dark2_r')\n",
        "plt.xlabel('Vehicle Age', fontsize = 15)\n",
        "plt.ylabel('Count', fontsize = 15)\n",
        "plt.title('Vehicle Age and Customer Response analysis', fontsize = 19)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SwQtaCg19h8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Customers with vechicle age 1-2 years are more likely to interested as compared to the other two\n",
        "\n",
        "* Customers with with Vehicle_Age <1 years have very less chance of buying Insurance"
      ],
      "metadata": {
        "id": "WMKVrr2Co8h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x = 'Response', y ='Annual_Premium', data = df1)"
      ],
      "metadata": {
        "id": "1JankwCh9iAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  People who response have slightly higher annual premium"
      ],
      "metadata": {
        "id": "u8kSEwPuqoqy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNcsHxMbF-DI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20, 8))\n",
        "sns.heatmap(df1.corr(), annot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Target variable is not much affected by Vintage variable. we can drop least correlated variable."
      ],
      "metadata": {
        "id": "i6h_e4LBqDVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoding Object columns\n",
        "* changing categorical value to numerical values"
      ],
      "metadata": {
        "id": "EUd_mZDcqgma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Gender'] = df1['Gender'].map({'Female':1, 'Male':0})\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "06wp_gsyqfwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Vehicle_Age']= df1['Vehicle_Age'].map({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "rSvvrwoyqf0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Vehicle_Damage']=df1['Vehicle_Damage'].map({'Yes':1, 'No':0})\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "nIQzHCT0vyQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering and Feature Selection"
      ],
      "metadata": {
        "id": "IVDbR9xkmu2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation = df1.corr()\n",
        "correlation['Response'].sort_values(ascending = False)[1:]"
      ],
      "metadata": {
        "id": "ij1OeM93yz9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df1.drop(columns=['id','Driving_License','Policy_Sales_Channel','Vintage','Response'])# independent variable\n",
        "y = df1['Response']# dependent variable"
      ],
      "metadata": {
        "id": "70sDv5zojTaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill any numerical NaNs with mode()\n",
        "\n",
        "fill_mode = lambda col: col.fillna(col.mode())\n",
        "X = X.apply(fill_mode, axis=0)\n",
        "df1 = df1.apply(fill_mode, axis=0)"
      ],
      "metadata": {
        "id": "ZP3ilC7r8hZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building"
      ],
      "metadata": {
        "id": "yNSb8zTs1Pr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for imbalance in data\n",
        "df1['Response'].value_counts()"
      ],
      "metadata": {
        "id": "k4iAuPyljTf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+* We can clearly see that there is a huge difference between the data set.\n",
        "* Standard ML techniques such as Decision Tree and Logistic Regression have a bias towards the majority class, and they tend to ignore the minority class. So solving this issue we use resampling technique.\n"
      ],
      "metadata": {
        "id": "cj_NpsbapGqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Resampling\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_new,y_new= ros.fit_resample(X, y)\n",
        "\n",
        "print(\"After Random Over Sampling Of Minor Class Total Samples are :\", len(y_new))\n",
        "print('Original dataset shape {}'.format(Counter(y)))\n",
        "print('Resampled dataset shape {}'.format(Counter(y_new)))\n"
      ],
      "metadata": {
        "id": "CfVYkVvKPePk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data in train and test sets"
      ],
      "metadata": {
        "id": "hxZSkqHTvSCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test ,y_train, y_test=  train_test_split(X_new, y_new, random_state=42, test_size=0.3)\n",
        "X_train.shape, X_test.shape , y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "LqfFmzFkxUoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the Dataset using Standard Scaling Technique.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "N4JMuxtAQr9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "enxfxI_PAkPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.CSV')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Response', axis=1)  # Features\n",
        "y = data['Response']               # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data using ColumnTransformer and Pipeline\n",
        "categorical_features = ['Gender']  # Add more categorical columns if needed\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Instantiate the LogisticRegression model\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42))\n",
        "])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "classification_rep = classification_report(y_test, pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "id": "G_kYObX8xUsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model** **Evaluation**"
      ],
      "metadata": {
        "id": "wlkDPz3FwXKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "precision = precision_score(y_test, pred)\n",
        "recall = recall_score(y_test, pred)\n",
        "f1 = f1_score(y_test, pred)\n",
        "conf_matrix = confusion_matrix(y_test, pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "id": "-JPolsOmc3Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Assuming you have trained and predicted using your model\n",
        "# Replace 'model' with your trained logistic regression model\n",
        "prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label='Logistic Regression')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pp55Vpp_r4R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**confusion_matrix**"
      ],
      "metadata": {
        "id": "Db8u8ZG7rHgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix= confusion_matrix(y_test, pred)\n",
        "print(matrix)\n",
        "sns.heatmap(matrix ,annot=True, fmt='g')"
      ],
      "metadata": {
        "id": "f0QQ1je0xUvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the confusion matrix we see that the model is predicting positive responses but also predicting negative response too."
      ],
      "metadata": {
        "id": "yeDxFIxbGuMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(pred, y_test))"
      ],
      "metadata": {
        "id": "fUBJ9Un8xUyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RandomForest Classifier"
      ],
      "metadata": {
        "id": "S8vUr5C4inMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.CSV')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Response', axis=1)  # Features\n",
        "y = data['Response']               # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data using OneHotEncoder for categorical variables\n",
        "categorical_features = ['Gender']  # Add more categorical columns if needed\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Instantiate the RandomForestClassifier model\n",
        "RF_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create a pipeline with preprocessing and modeling steps\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RF_model)\n",
        "])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "rf_pred = model.predict(X_test)\n",
        "\n",
        "# Obtain prediction probabilities\n",
        "rf_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, rf_pred)\n",
        "classification_rep = classification_report(y_test, rf_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "id": "Ukoc7CQkxU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "i_FvcFN1q0_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "r_rf=  recall_score(y_test, rf_pred)\n",
        "print(\"recall_score : \", r_rf)\n",
        "\n",
        "p_rf= precision_score(y_test, rf_pred)\n",
        "print(\"precision_score :\",p_rf)\n",
        "\n",
        "f1_rf= f1_score(y_test, rf_pred)\n",
        "print(\"f1_score :\", f1_rf)\n",
        "\n",
        "A_rf= accuracy_score(y_test, rf_pred)\n",
        "print(\"accuracy_score :\",A_rf)\n",
        "\n",
        "acu_rf = roc_auc_score(rf_pred, y_test)\n",
        "print(\"ROC_AUC Score:\",acu_rf)"
      ],
      "metadata": {
        "id": "uZSBq2BCxU8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, _ = roc_curve(y_test, rf_proba)\n",
        "\n",
        "plt.title('Random Forest ROC curve')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.plot(fpr,tpr)\n",
        "plt.plot((0,1), linestyle=\"--\",color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dnn6FqmGvOK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**confusion_matrix**"
      ],
      "metadata": {
        "id": "xnOTl7agq6Pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix= confusion_matrix(y_test,rf_pred)\n",
        "print(matrix)\n",
        "sns.heatmap(matrix ,annot=True, fmt='g')"
      ],
      "metadata": {
        "id": "wf5hLuozbBwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix now shows that the model now is much better with predicting positive responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "YsPdPJ5EN6o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(rf_pred, y_test))"
      ],
      "metadata": {
        "id": "4LTWhMhkbByN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs very well, so we can use it to predict unknown data."
      ],
      "metadata": {
        "id": "B3TMrdj3N_La"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost"
      ],
      "metadata": {
        "id": "u2xkZKrAqFw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Response', axis=1)\n",
        "y = data['Response']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data using OneHotEncoder for categorical variables\n",
        "categorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']  # Add more categorical columns if needed\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Instantiate the XGBClassifier model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Create a pipeline with preprocessing and modeling steps\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb_model)\n",
        "])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "xgb_pred = model.predict(X_test)\n",
        "\n",
        "# Obtain prediction probabilities\n",
        "xgb_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, xgb_pred)\n",
        "classification_rep = classification_report(y_test, xgb_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "id": "nUPXIaN-bB0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model** **Evaluation**"
      ],
      "metadata": {
        "id": "DBKtqu2yq99P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score, f1_score, roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.CSV')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Response', axis=1)\n",
        "y = data['Response']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data using OneHotEncoder for categorical variables\n",
        "categorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Instantiate the XGBClassifier model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Create a pipeline with preprocessing and modeling steps\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', xgb_model)\n",
        "])\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "XG_pred = model.predict(X_test)\n",
        "\n",
        "# Obtain prediction probabilities\n",
        "XG_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model\n",
        "r_XG = recall_score(y_test, XG_pred)\n",
        "print(\"recall_score:\", r_XG)\n",
        "\n",
        "p_XG = precision_score(y_test, XG_pred)\n",
        "print(\"precision_score:\", p_XG)\n",
        "\n",
        "f1_XG = f1_score(y_test, XG_pred)\n",
        "print(\"f1_score:\", f1_XG)\n",
        "\n",
        "A_XG = accuracy_score(y_test, XG_pred)\n",
        "print(\"accuracy_score:\", A_XG)\n",
        "\n",
        "acu_XG = roc_auc_score(y_test, XG_proba)\n",
        "print(\"ROC_AUC Score:\", acu_XG)\n"
      ],
      "metadata": {
        "id": "rkSS4_robB3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, XG_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label='XGBoost')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lhDpZV46vp-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**confusion_matrix**"
      ],
      "metadata": {
        "id": "5UhMliX-rNgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix= confusion_matrix(y_test,XG_pred)\n",
        "print(matrix)\n",
        "sns.heatmap(matrix ,annot=True, fmt='g')"
      ],
      "metadata": {
        "id": "y7cPsD81bB5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the confusion matrix we see that the model is a bit better with predicting positive responses."
      ],
      "metadata": {
        "id": "5xmpJPxTPCyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(XG_pred, y_test))"
      ],
      "metadata": {
        "id": "eDcDaKxsbB9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing  the Model\n",
        "\n"
      ],
      "metadata": {
        "id": "-AIAbTVu2WYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column names\n",
        "for col in X_train.columns:\n",
        "    if '[' in col or ']' in col or '<' in col:\n",
        "        new_col_name = col.replace('[', '_').replace(']', '_').replace('<', '_')\n",
        "        X_train.rename(columns={col: new_col_name}, inplace=True)\n",
        "\n",
        "# Check column names\n",
        "for col in X_test.columns:\n",
        "    if '[' in col or ']' in col or '<' in col:\n",
        "        new_col_name = col.replace('[', '_').replace(']', '_').replace('<', '_')\n",
        "        X_test.rename(columns={col: new_col_name}, inplace=True)\n",
        "\n",
        "# Now proceed with training your XGBoost model\n"
      ],
      "metadata": {
        "id": "rZd5qFQe2Um8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "PgnI-SVNZLVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* In this project, we aimed to predict whether customers would be interested in purchasing health insurance, utilizing machine learning techniques. The dataset provided valuable insights into various customer attributes that could influence their decision. We employed three different classification models - Logistic Regression, Random Forest, and XGBoost - to build predictive models. The evaluation of these models included key performance metrics such as accuracy, recall, precision, f1-score, and ROC AUC.\n",
        "\n",
        "* Our analysis revealed that the XGBoost model outperformed the other two models in terms of accuracy, recall, precision, and f1-score. This indicates that XGBoost is particularly effective at capturing true positives, minimizing false negatives, and providing an overall better balance between precision and recall. Additionally, the ROC AUC score for the XGBoost model indicated its strong discriminative power in distinguishing between positive and negative cases.\n",
        "\n",
        "* The success of the project can be attributed to several factors, including the careful preprocessing of data, feature engineering, and selecting appropriate models. The one-hot encoding of categorical variables, standardization of numerical features, and appropriate train-test splitting ensured the models' accuracy and generalization to new data.\n",
        "\n",
        "* It is important to note that while the models demonstrated promising results, the ultimate application of the models should be accompanied by careful consideration of business context, potential risks, and cost-benefit analysis. Moreover, continuous monitoring and periodic model updates are essential to maintain their relevance and effectiveness over time.\n",
        "\n",
        "* In conclusion, this project showcases the effectiveness of machine learning techniques in predicting customer behavior, specifically in the context of health insurance cross-selling. The results obtained can provide valuable insights to insurance companies in optimizing their marketing strategies and enhancing customer engagement."
      ],
      "metadata": {
        "id": "NYK6KTthZOQL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W1jkdo37rNej"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}